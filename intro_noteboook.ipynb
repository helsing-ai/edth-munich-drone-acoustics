{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392ad211",
   "metadata": {},
   "source": [
    "# Acoustic signature challenge\n",
    "\n",
    "This notebook introduces concepts which may be useful for participation in Helsing's acoustic signature challenge.\n",
    "\n",
    "It aims to provide practical information on how to\n",
    "- visualise and process acoustic data;\n",
    "- datasets for machine learning;\n",
    "- train a simple classifier to detect drones and helicopters in audio data.\n",
    "\n",
    "### Table of contents\n",
    "* [Processing audio signals](#processing-audio-signals)\n",
    "    * [Visualing audio signals](#visualising-audio-signals)\n",
    "    * [Extracting audio features](#extracting-audio-features)\n",
    "* [Baseline classification](#baseline-classification)\n",
    "    * [Creating datasets](#creating-datasets)\n",
    "    * [Training a basic classifier](#training-a-simple-classifier)\n",
    "    * [Evaluating on the test set](#evaluating-on-the-validation-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d3729-33e4-4adc-8173-89239a41e496",
   "metadata": {},
   "source": [
    "## Processing audio signals\n",
    "### Visualising audio signals\n",
    "\n",
    "Audio waveforms are time series and as such they are fully described by the combination of raw data and the rate at which they were sampled. To ensure these quantities are matched in our analysis, we provide in `base.py` a custom `AudioWaveform` dataclass to hold audio waveforms:\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class AudioWaveform:\n",
    "    data: Tensor\n",
    "    sample_rate: float\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> AudioWaveform:\n",
    "        data, samplerate = sf.read(path)\n",
    "        return AudioWaveform(torch.as_tensor(data, dtype=torch.float32), samplerate)\n",
    "\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        return self.data.shape[-1] / self.sample_rate\n",
    "```\n",
    "\n",
    "With this representation in mind, we can start looking at examples waveforms from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "b45e9802",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hs_hackathon_drone_acoustics import EXAMPLES_DIR\n",
    "from hs_hackathon_drone_acoustics.base import AudioWaveform  # Custom waveform representation.\n",
    "from hs_hackathon_drone_acoustics.plot import plot_waveform  # Custom function used to plot waveforms.\n",
    "\n",
    "example_files = list(EXAMPLES_DIR.glob(\"*.wav\"))\n",
    "fig, axes = plt.subplots(1, len(example_files), figsize=(16, 3))\n",
    "for ifile, example_file in enumerate(example_files):\n",
    "    waveform = AudioWaveform.load(example_file)\n",
    "    plot_waveform(waveform, axis=axes[ifile])\n",
    "    axes[ifile].set_title(example_file.name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a16797b3-fad2-40a8-b1b2-94be3c7e307e",
   "metadata": {},
   "source": [
    "From these examples, we can already see differences between the different classes: the helicopter and drones classes appear to have white noise-like contents throughout, while the background has periods of silence.\n",
    "\n",
    "\n",
    "To verify this observation, it is useful to plot the variations in a waveform's frequency components over time, also known as a **spectrogram**. Spectrograms are obtained by taking the magnitude of a [Short-Time Fourier Transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform). Code to plot a spectrogram from a wavform is provided in the `hs_hackathon_drone_acoustics.plot.plot_spectrogram` function."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0ec92c9",
   "metadata": {},
   "source": [
    "from hs_hackathon_drone_acoustics.plot import plot_spectrogram  # Custom function to plot spectrograms.\n",
    "\n",
    "fig, axes = plt.subplots(1, len(example_files), figsize=(16, 3))\n",
    "for ifile, example_file in enumerate(example_files):\n",
    "    waveform = AudioWaveform.load(example_file)\n",
    "    plot_spectrogram(waveform, axis=axes[ifile])\n",
    "    axes[ifile].set_title(example_file.name)\n",
    "    axes[ifile].set_ylim([0, 4])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80f5560c",
   "metadata": {},
   "source": [
    "### Extracting audio features\n",
    "\n",
    "Clearly, the various classes have different waveform characteristics. It may be useful for classification to extract waveform features which can capture and summarise differentiating characteristics for each class. For example, we may want to extract the waveform's energy."
   ]
  },
  {
   "cell_type": "code",
   "id": "517f43bb",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_waveform_energy(waveform: AudioWaveform) -> float:\n",
    "    energy = np.linalg.vector_norm(waveform.data, ord=2)\n",
    "    return energy\n",
    "\n",
    "\n",
    "for example_file in example_files:\n",
    "    waveform = AudioWaveform.load(example_file)\n",
    "    energy = extract_waveform_energy(waveform)\n",
    "    print(f\"Energy in {example_file.name}: {energy}.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3a2594a",
   "metadata": {},
   "source": [
    "Another very common set of features to extract in audio processing are [Mel-frequency cepstral coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). They capture frequency information in the signal in a more condensed manner than the spectrogram above."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce23deb4",
   "metadata": {},
   "source": [
    "import librosa\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "def extract_mean_mfccs(waveform: AudioWaveform, n_mfcc: int = 20) -> NDArray[np.float64]:\n",
    "    mfccs = librosa.feature.mfcc(y=waveform.data.numpy(), sr=waveform.sample_rate, n_mfcc=n_mfcc)\n",
    "    mean_mfccs = np.mean(mfccs, axis=1)\n",
    "    return mean_mfccs\n",
    "\n",
    "\n",
    "for example_file in example_files:\n",
    "    waveform = AudioWaveform.load(example_file)\n",
    "    mfccs = extract_mean_mfccs(waveform)\n",
    "    print(f\"Mean MFCC[0:5] for {example_file.name}: {mfccs[0:5]}.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75c5811f",
   "metadata": {},
   "source": [
    "These features can be used to classify audio waveforms: e.g. the energy in the background class is lower than in the helicopter class, which itself is lower than in the drone class. This is the case for the selected examples but may not be true when considering the broader dataset. In the next section, we will show how features can be extracted from entire datasets and used to train a simple classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26186565",
   "metadata": {},
   "source": [
    "## Baseline classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c63177",
   "metadata": {},
   "source": [
    "### Creating datasets\n",
    "\n",
    "To manage your dataset structure, we provide in `base.py` a custom `AudioDataset` class which loads waveforms from files and associates them with the correct label. Make sure you have followed the data download guide in Phase 1: Model Training in the README!"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dc23ccd",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "from hs_hackathon_drone_acoustics import CLASSES, RAW_DATA_DIR\n",
    "from hs_hackathon_drone_acoustics.base import AudioDataset\n",
    "\n",
    "TRAIN_PATH = RAW_DATA_DIR / \"train\"\n",
    "VAL_PATH = RAW_DATA_DIR / \"val\"\n",
    "\n",
    "train_dataset = AudioDataset(root_dir=TRAIN_PATH)\n",
    "val_dataset = AudioDataset(root_dir=VAL_PATH)\n",
    "\n",
    "# Let's plot some examples from the training dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 3))\n",
    "for ax in axes:\n",
    "    idx = random.randint(0, len(train_dataset))\n",
    "    waveform, label = train_dataset[idx]\n",
    "    plot_spectrogram(waveform, ax)\n",
    "    ax.set_title(f\"Train item {idx}, class = {CLASSES[label]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "817c4a0f",
   "metadata": {},
   "source": [
    "### Training a simple classifier\n",
    "\n",
    "In this section we use `sklearn` to setup a classifier using [Stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). The classifier learns to separate training samples from our three classes (background, drone, and helicopter) using the features explored above.\n",
    "\n",
    "To simplify the data processing pipeline, we provide `FeatureExtractors` and an `sklearn` feature extractor pipeline in `feature_extractors.py`."
   ]
  },
  {
   "cell_type": "code",
   "id": "763552cc",
   "metadata": {},
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hs_hackathon_drone_acoustics.feature_extractors import EnergyFeatureExtractor, FeatureExtractorPipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = make_pipeline(\n",
    "    FeatureExtractorPipeline(EnergyFeatureExtractor()),  # Extract features\n",
    "    StandardScaler(),  # Apply normalisation\n",
    "    SGDClassifier(max_iter=1000, tol=1e-3, random_state=0),\n",
    ")\n",
    "\n",
    "train_waveforms, train_labels = train_dataset[:]\n",
    "print(\"Fitting model on the training set ...\")\n",
    "model.fit(train_waveforms, train_labels)\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2f135c3",
   "metadata": {},
   "source": [
    "### Evaluating on the validation set\n",
    "\n",
    "We know want to evaluate the performance of our classifier. We provide an evaluation script in `metrics.py` which computes\n",
    "- the cross-entropy loss between ground-truth labels and model prediction;\n",
    "- the model accuracy;\n",
    "- the model confusion matrix.\n",
    "\n",
    "We should calculate these metrics for both the training set and a set of unseen data, the validation set. Performance on the training set indicates how much the model has learned from available data, whereas performance on the validation test gives information on how well the model generalises to unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "fe09afca",
   "metadata": {},
   "source": [
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch import Tensor\n",
    "\n",
    "from hs_hackathon_drone_acoustics.metrics import evaluate, get_confusion_matrix_str\n",
    "\n",
    "\n",
    "def predict_and_evaluate(model: Pipeline, dataset: AudioDataset) -> tuple[float, float, Tensor]:\n",
    "    waveforms, targets = dataset[:]\n",
    "    all_preds = model.predict(waveforms)\n",
    "    # Convert predictions to probabilities so we can compute a loss.\n",
    "    all_probas = torch.zeros(len(all_preds), len(CLASSES))\n",
    "    for i, pred in enumerate(all_preds):\n",
    "        all_probas[i, pred] = 1\n",
    "    return evaluate(all_probas, torch.as_tensor(targets))\n",
    "\n",
    "\n",
    "train_loss, train_accuracy, train_confusion_matrix = predict_and_evaluate(model, train_dataset)\n",
    "val_loss, val_accuracy, val_confusion_matrix = predict_and_evaluate(model, val_dataset)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Validation accuracy: {val_accuracy:.3f}\")\n",
    "print(f\"Training confusion matrix: \\n{get_confusion_matrix_str(train_confusion_matrix)}\")\n",
    "print(f\"Validation confusion matrix: \\n{get_confusion_matrix_str(val_confusion_matrix)}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hs-hackathon-drone-acoustics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
